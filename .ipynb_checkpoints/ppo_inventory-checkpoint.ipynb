{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< Updated upstream
=======
   "id": "billion-annual",
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch as th\n",
    "import numpy as np\n",
    "import inventory_model\n",
    "import pandas as pd\n",
    "from evaluate import *\n",
    "from ppo_evaluate import ppo_evaluate\n",
<<<<<<< Updated upstream
    "import matplotlib as plt\n",
=======
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
>>>>>>> Stashed changes
    "\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< Updated upstream
=======
   "id": "introductory-hormone",
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< Updated upstream
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrapper for cont env, plot result for several steps\n",
    "def ppo_eval_interval(p, L, t_t, n_iter, learning_rate=0.0003):\n",
    "    ContCONFIG = {'h': 1, 'p': p, 'L': L, 'lambda': 1}\n",
    "    cont_env = make_vec_env('inventory_cont_config_fix_model-v0', n_envs=4, env_kwargs=ContCONFIG)\n",
    "    print(\"Running PPO w/: p=\", p, \", L=\",L)\n",
    "    cont_model = PPO(MlpPolicy, cont_env, verbose=1, gamma = 1, learning_rate = learning_rate,use_sde = False)\n",
=======
   "id": "demonstrated-calculation",
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrapper for cont env with PPO, plot result for several steps\n",
    "def ppo_eval_interval(p, L, t_t, n_iter, n_step, gae, learning_rate, bias):\n",
    "    ContCONFIG = {'h': 1, 'p': p, 'L': L, 'lambda': 1, 'action':20}\n",
    "    #PolicyCONFIG = dict(activation_fn=th.nn.Sigmoid,\n",
    "                    # net_arch=[dict(pi=[16, 16], vf=[64,64,64])])\n",
    "    cont_env = make_vec_env('inventory_cont_config_fix_model-v0', n_envs=4, env_kwargs=ContCONFIG)\n",
    "    print(\"Running PPO w/: p=\", p, \", L=\",L)\n",
    "    cont_model = PPO(MlpPolicy, cont_env, verbose=0, gamma = 1, gae_lambda=gae,\n",
    "                     learning_rate = learning_rate,use_sde = False, n_steps = n_step)\n",
    "    cont_model.set_parameters({'policy': OrderedDict([\n",
    "                         ('action_net.weight', th.zeros(1,64)), \n",
    "                         ('action_net.bias', th.tensor([bias])),])},exact_match=False)\n",
>>>>>>> Stashed changes
    "    env_eval = make_vec_env('inventory_cont_config_fix_model-v0', n_envs=1, env_kwargs=ContCONFIG)\n",
    "    timesteps = 0\n",
    "    numiter = n_iter#test\n",
    "    res_mean_arr = []\n",
    "    res_std_arr = []\n",
    "\n",
    "    while(timesteps <= t_t):\n",
    "\n",
<<<<<<< Updated upstream
    "        cont_model.learn(total_timesteps=8000)#each iteration has 8192 timesteps with n_env=4\n",
    "        timesteps = timesteps + 8192\n",
=======
    "        cont_model.learn(total_timesteps=4*n_step-1)\n",
    "        timesteps = timesteps + 4*n_step\n",
>>>>>>> Stashed changes
    "\n",
    "        res_mean, res_std = ppo_evaluate(cont_model, env_eval, numiter)\n",
    "        res_mean_arr.append(-res_mean)\n",
    "        res_std_arr.append(res_std)\n",
<<<<<<< Updated upstream
    "        print(res_mean_arr)\n",
    "    \n",
    "#     plt.pyplot.plot(res_mean_arr)\n",
    "\n",
    "    return res_mean_arr, res_std_arr"
=======
    "        \n",
    "        if -res_mean == min(res_mean_arr):\n",
    "            cont_model.save(\"ppo_min_model_\"+str(p)+\"_\"+str(L))\n",
    "            \n",
    "#     plt.plot(res_mean_arr,label=\"gae_lambda=\"+str(gae))\n",
    "#     plt.xlabel(\"Iteration\")\n",
    "#     #plt.axhline(y=13.10673598, color='r', linestyle='-',label=\"optimal constant-order policy\")\n",
    "#     plt.ylabel(\"Average cost\")\n",
    "#     plt.title(\"L=\"+str(L)+\", p=\"+str(p))\n",
    "#     plt.show()\n",
    "    \n",
    "    min_model = PPO.load(\"ppo_min_model_\"+str(p)+\"_\"+str(L))\n",
    "    mean_min, std_min = ppo_evaluate(min_model, env_eval, 50000)\n",
    "    print(\"p=\"+str(p)+\"ï¼Œ L=\"+str(L)+\": mean \"+str(-mean_min)+\", std_dev: \"+str(std_min))\n",
    "    \n",
    "    return res_mean_arr, res_std_arr, mean_min, std_min"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< Updated upstream
=======
   "id": "entitled-suffering",
>>>>>>> Stashed changes
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
<<<<<<< Updated upstream
    "# listp = [0.25,1,4,9,39,99]\n",
    "# listL = [30,50,70,100]\n",
    "listp = [99]\n",
    "listL = [100]\n",
    "t_t = 200000\n",
    "n_iter = 1000\n",
=======
    "#listp = [0.25,1,4,9,39,99]\n",
    "#listL = [1,4,10,20,30,50,70,100]\n",
    "#setting the parameters\n",
    "p=99\n",
    "L=1\n",
    "n_iter = 10000\n",
    "#gae lamda: suggest choose 0.95 when L<= 30, choose 0.99 when L>=50\n",
    "gae_lambda = 0.95\n",
>>>>>>> Stashed changes
    "learning_rate = 0.0003\n",
    "ppo_res = pd.DataFrame(columns = ['p','L','res_mean', 'res_std'])\n",
    "#bias suggest value: when p=0.25, bias=0.1; when p=1, bias=0.2; when p=4, bias=0.3; \n",
    "#when p=9, bias=0.4; when p=39, bias=0.5;when p=99, bias=0.6.\n",
    "bias=0.6\n",
    "# for p in listp:\n",
    "#     for L in listL:\n",
    "\n",
<<<<<<< Updated upstream
    "for p in listp:\n",
    "    for L in listL:\n",
    "        res_mean, res_std = ppo_eval_interval(p,L,t_t, n_iter, learning_rate)\n",
    "        ppo_res = ppo_res.append({'p': p, 'L':L, 'res_mean':min(res_mean), 'res_std': res_std[np.argmin(res_mean)]}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_res"
=======
    "n_step = 2048 \n",
    "t_t = 20*4*n_step\n",
    "res_mean, res_std, mean_min, std_min = ppo_eval_interval(p,L,t_t, n_iter, n_step, gae_lambda, learning_rate,bias)\n",
    "ppo_res = ppo_res.append({'p': p, 'L':L, 'res_mean':-mean_min, 'res_std': std_min}, ignore_index=True)"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< Updated upstream
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.pyplot.plot(res_mean)"
   ]
  },
  {
   "cell_type": "markdown",
=======
   "id": "broadband-agency",
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [],
   "source": [
    "#codes for plotting the policy when L=1\n",
    "min_model=PPO.load(\"results/ppo_min_model_\"+str(1)+\"_\"+str(1))\n",
    "a=range(0,50)\n",
    "b=[0.1*i for i in a]\n",
    "graph=np.zeros([50,50])\n",
    "for i in range(50):\n",
    "    for j in range(50):\n",
    "        graph[i,j]=min_model.predict(([b[i]],[b[j]]),deterministic=True)[0]\n",
    "X, Y = np.meshgrid(b, b)\n",
    "plt.contourf(Y, X, graph, 9, cmap=plt.cm.viridis)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"On-hand Inventory\")\n",
    "plt.ylabel(\"Pipeline Inventory\")\n",
    "plt.title(\"PPO Policy Action for L=1, p=1\")\n",
    "print (graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wrapper for continuous environment, obsolete\n",
    "def ppo_eval(p, L, t_t, n_iter, learning_rate=0.0003):\n",
    "    ContCONFIG = {'h': 1, 'p': p, 'L': L, 'lambda': 1}\n",
    "    cont_env = make_vec_env('inventory_cont_config_fix_model-v0', n_envs=4, env_kwargs=ContCONFIG)\n",
    "    print(\"Running PPO w/: p=\", p, \", L=\",L)\n",
    "    cont_model = PPO(MlpPolicy, cont_env, verbose=1, gamma = 1, learning_rate = learning_rate,use_sde = False)\n",
    "    cont_model.learn(total_timesteps=t_t)#testing 2000\n",
    "\n",
    "    cont_model.save(\"inv_cont_2\")\n",
    "    trained_model = PPO.load(\"inv_cont_2\")\n",
    "    env_eval = make_vec_env('inventory_cont_config_fix_model-v0', n_envs=1, env_kwargs=ContCONFIG)\n",
    "    numiter = n_iter#test\n",
    "    res_mean, res_std = ppo_evaluate(trained_model, env_eval, numiter)\n",
    "    print(-res_mean,'+/=',1.96*res_std/np.sqrt(5)) \n",
    "\n",
    "    return res_mean, res_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# listp = [0.25,1,4,9,39,99]\n",
    "# listL = [1,4,10,20,30,50,70,100]\n",
    "listp = [39]\n",
    "listL = [1]\n",
    "t_t = 200000\n",
    "n_iter = 100000\n",
    "learning_rate = 0.005\n",
    "ppo_res = pd.DataFrame(columns = ['p','L','res_mean', 'res_std'])\n",
    "\n",
    "for p in listp:\n",
    "    for L in listL:\n",
    "        res_mean, res_std = ppo_eval(p,L,t_t, n_iter, learning_rate)\n",
    "        ppo_res = ppo_res.append({'p': p, 'L':L, 'res_mean':res_mean, 'res_std': res_std}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#continuous model try with cont action\n",
    "\n",
    "# #env = make_vec_env('inventory_cont_model-v0', ContCONFIG = {'h': 1, 'p': 1, 'L': 10, 'lambda': 1}, n_envs=4)\n",
    "# #how to set parameters???\n",
    "# cont_env = make_vec_env('inventory_cont_model-v0', n_envs=8)\n",
    "# cont_model = PPO(MlpPolicy, cont_env, verbose=1, gamma = 1)\n",
    "# cont_model.learn(total_timesteps=200000)\n",
    "# # fixed issue with vector, now action can be continuous\n",
    "\n",
    "# #continuous evaluation\n",
    "# cont_model.save(\"inv_cont_1\")\n",
    "# trained_model = PPO.load(\"inv_cont_1\")\n",
    "# env1 = gym.make('inventory_cont_model-v0')\n",
    "# numiter = 50000\n",
    "# res_mean, res_std = ppo_evaluate(trained_model, env1, numiter)\n",
    "# print(-res_mean,'+/=',1.96*res_std/np.sqrt(5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discrete environment\n",
    "# #h=1,p=99,L=1,max_inventory=100,max_action=100,binomial(6,0.5)\n",
    "# env = make_vec_env('inventory_model-v0', n_envs=4)\n",
    "# model = PPO(MlpPolicy, env, verbose=1, gamma = 1)\n",
    "# model.learn(total_timesteps=1000000)\n",
    "# model.save(\"inv_2\")\n",
    "\n",
    "# trained_model2 = PPO.load(\"inv_2\")\n",
    "# env2 = gym.make('inventory_model-v0')\n",
    "# numiter = 200000\n",
    "# res_mean, res_std = ppo_evaluate(trained_model2, env2, numiter)\n",
    "# print(-res_mean,'+/=',1.96*res_std/np.sqrt(5)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discrete environment 2\n",
    "#h=1,p=99,L=1,max_inventory=100,max_action=6,binomial(6,0.5)\n",
    "# env = make_vec_env('inventory_model-v0', n_envs=4)\n",
    "# model = PPO(MlpPolicy, env, verbose=1, gamma = 1)\n",
    "# model.learn(total_timesteps=1000000)\n",
    "# model.save(\"inv_3\")\n",
    "\n",
    "# #set action space to be small([0,1,2,3,4,5,6]) seems to get reasonable results...\n",
    "# trained_model3 = PPO.load(\"inv_3\")\n",
    "# env3 = gym.make('inventory_model-v0')\n",
    "# numiter = 200000\n",
    "# res_mean, res_std = ppo_evaluate(trained_model3, env3, numiter)\n",
    "# print(-res_mean,'+/=',1.96*res_std/np.sqrt(5)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
